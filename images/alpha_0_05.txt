/home/snowkylin/anaconda2/envs/tensorflow/bin/python /home/snowkylin/桌面/rnn_vae/train.py
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 1060 6GB
major: 6 minor: 1 memoryClockRate (GHz) 1.8095
pciBusID 0000:01:00.0
Total memory: 5.96GiB
Free memory: 5.54GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)
epoch 0
batches 0, loss 66.4692 (re: 590.877, kl: 38.8688)
batches 100, loss 10.1487 (re: 202.289, kl: 0.0360871)
batches 200, loss 10.2023 (re: 203.75, kl: 0.0155664)
batches 300, loss 10.4072 (re: 207.98, kl: 0.00865278)
batches 400, loss 10.2576 (re: 205.045, kl: 0.00563089)
batches 500, loss 10.6366 (re: 212.651, kl: 0.00423395)
batches 600, loss 10.5234 (re: 210.404, kl: 0.00336214)
batches 700, loss 10.182 (re: 203.594, kl: 0.00236414)
batches 800, loss 9.94644 (re: 198.893, kl: 0.00188843)
batches 900, loss 10.1809 (re: 203.588, kl: 0.00157997)
epoch 1
batches 0, loss 10.8534 (re: 217.041, kl: 0.00137386)
batches 100, loss 10.4937 (re: 209.851, kl: 0.00121542)
batches 200, loss 10.6516 (re: 213.014, kl: 0.000984039)
batches 300, loss 10.6543 (re: 213.067, kl: 0.000928345)
batches 400, loss 10.4957 (re: 209.899, kl: 0.000784435)
batches 500, loss 10.3096 (re: 206.179, kl: 0.000676918)
batches 600, loss 10.0915 (re: 201.818, kl: 0.000622959)
batches 700, loss 10.2284 (re: 204.558, kl: 0.000548897)
batches 800, loss 10.1975 (re: 203.94, kl: 0.000514298)
batches 900, loss 10.3429 (re: 206.849, kl: 0.000471764)
epoch 2
batches 0, loss 10.2211 (re: 204.414, kl: 0.000415134)
batches 100, loss 10.6466 (re: 212.923, kl: 0.000413189)
batches 200, loss 10.4113 (re: 208.218, kl: 0.000373402)
batches 300, loss 10.3391 (re: 206.776, kl: 0.000325603)
batches 400, loss 10.0966 (re: 201.924, kl: 0.000442009)
batches 500, loss 10.2895 (re: 205.782, kl: 0.000394344)
batches 600, loss 10.0519 (re: 201.034, kl: 0.000274811)
batches 700, loss 10.225 (re: 204.495, kl: 0.000296631)
batches 800, loss 10.2537 (re: 205.069, kl: 0.000291328)
batches 900, loss 10.2681 (re: 205.358, kl: 0.000243225)
epoch 3
batches 0, loss 10.5132 (re: 210.257, kl: 0.000339489)
batches 100, loss 9.99918 (re: 199.977, kl: 0.000351944)
batches 200, loss 10.1447 (re: 202.889, kl: 0.000231628)
batches 300, loss 10.6325 (re: 212.645, kl: 0.000216503)
batches 400, loss 10.3378 (re: 206.751, kl: 0.000213833)
batches 500, loss 10.4178 (re: 208.35, kl: 0.000282154)
batches 600, loss 10.3693 (re: 207.381, kl: 0.000238342)
batches 700, loss 10.4452 (re: 208.9, kl: 0.000260315)
batches 800, loss 10.3155 (re: 206.308, kl: 0.00016613)
batches 900, loss 10.3791 (re: 207.575, kl: 0.000390587)
epoch 4
batches 0, loss 10.5449 (re: 210.894, kl: 0.000217495)
batches 100, loss 10.094 (re: 201.877, kl: 0.00016552)
batches 200, loss 10.8621 (re: 217.238, kl: 0.000247955)
batches 300, loss 10.3902 (re: 207.799, kl: 0.000220108)
batches 400, loss 10.37 (re: 207.396, kl: 0.000204029)
batches 500, loss 10.7049 (re: 214.095, kl: 0.000136795)
batches 600, loss 10.297 (re: 205.939, kl: 0.00012701)
batches 700, loss 9.98939 (re: 199.785, kl: 0.000139904)
batches 800, loss 10.4887 (re: 209.77, kl: 0.000238628)
batches 900, loss 10.2674 (re: 205.342, kl: 0.000297413)
epoch 5
batches 0, loss 10.2071 (re: 204.139, kl: 0.000194359)
batches 100, loss 9.9729 (re: 199.455, kl: 0.000153427)
batches 200, loss 10.1242 (re: 202.481, kl: 0.000112762)
batches 300, loss 9.56503 (re: 191.297, kl: 0.000216293)
batches 400, loss 10.0901 (re: 201.8, kl: 0.000128136)
batches 500, loss 10.3583 (re: 207.162, kl: 0.000153294)
batches 600, loss 9.82186 (re: 196.434, kl: 0.000185261)
batches 700, loss 10.119 (re: 202.378, kl: 0.000138054)
batches 800, loss 10.5722 (re: 211.44, kl: 0.000219669)
batches 900, loss 10.1556 (re: 203.109, kl: 0.000164165)
epoch 6
batches 0, loss 10.7177 (re: 214.35, kl: 0.000196762)
batches 100, loss 10.2463 (re: 204.913, kl: 0.000616722)
batches 200, loss 10.2897 (re: 205.792, kl: 0.000105343)
batches 300, loss 9.97546 (re: 199.508, kl: 9.1629e-05)
batches 400, loss 9.87893 (re: 197.576, kl: 0.00011776)
batches 500, loss 10.1589 (re: 203.174, kl: 0.00018713)
batches 600, loss 10.3745 (re: 207.489, kl: 9.29451e-05)
batches 700, loss 10.2813 (re: 205.624, kl: 7.35283e-05)
batches 800, loss 10.6451 (re: 212.892, kl: 0.00046772)
batches 900, loss 10.6509 (re: 213.016, kl: 0.00012661)
epoch 7
batches 0, loss 10.4211 (re: 208.419, kl: 0.000105209)
batches 100, loss 10.7067 (re: 214.125, kl: 0.000493393)
batches 200, loss 10.1747 (re: 203.49, kl: 0.000189228)
batches 300, loss 10.595 (re: 211.897, kl: 0.000183563)
batches 400, loss 10.2527 (re: 205.052, kl: 9.9659e-05)
batches 500, loss 10.2177 (re: 204.353, kl: 0.000115108)
batches 600, loss 10.6843 (re: 213.682, kl: 0.000185127)
batches 700, loss 10.2995 (re: 205.985, kl: 0.000220146)
batches 800, loss 10.6377 (re: 212.753, kl: 0.000109901)
batches 900, loss 10.3417 (re: 206.828, kl: 0.000251198)
epoch 8
batches 0, loss 10.1593 (re: 203.182, kl: 0.000155277)
batches 100, loss 10.2465 (re: 204.928, kl: 0.000118504)
batches 200, loss 10.3766 (re: 207.529, kl: 8.61549e-05)
batches 300, loss 9.97151 (re: 199.429, kl: 7.47681e-05)
batches 400, loss 10.6287 (re: 212.572, kl: 0.000115776)
batches 500, loss 10.6476 (re: 212.949, kl: 9.62257e-05)
batches 600, loss 10.6461 (re: 212.921, kl: 0.000106544)
batches 700, loss 9.78126 (re: 195.624, kl: 8.6956e-05)
batches 800, loss 10.1193 (re: 202.385, kl: 7.58171e-05)
batches 900, loss 10.1791 (re: 203.581, kl: 7.15446e-05)
epoch 9
batches 0, loss 10.4368 (re: 208.732, kl: 0.000161858)
batches 100, loss 10.2589 (re: 205.172, kl: 0.00026598)
batches 200, loss 10.5229 (re: 210.455, kl: 0.000106201)
batches 300, loss 9.78651 (re: 195.729, kl: 7.77054e-05)
batches 400, loss 13.1341 (re: 207.686, kl: 2.89452)
batches 500, loss 10.9591 (re: 198.371, kl: 1.09535)
batches 600, loss 10.4573 (re: 203.737, kl: 0.284696)
batches 700, loss 10.2325 (re: 201.345, kl: 0.174006)
batches 800, loss 10.7439 (re: 210.183, kl: 0.247087)
batches 900, loss 10.5746 (re: 203.723, kl: 0.408846)
epoch 10
batches 0, loss 11.2091 (re: 205.595, kl: 0.978236)
batches 100, loss 10.421 (re: 206.318, kl: 0.110647)
batches 200, loss 10.4442 (re: 206.809, kl: 0.109214)
batches 300, loss 12.2741 (re: 207.418, kl: 2.00331)
batches 400, loss 11.1514 (re: 208.179, kl: 0.781544)
batches 500, loss 10.5855 (re: 198.442, kl: 0.698306)
batches 600, loss 10.713 (re: 207.706, kl: 0.344944)
batches 700, loss 10.5054 (re: 204.848, kl: 0.276771)
batches 800, loss 10.1145 (re: 198.498, kl: 0.199599)
batches 900, loss 10.5174 (re: 206.086, kl: 0.22437)
epoch 11
batches 0, loss 10.5909 (re: 207.593, kl: 0.222357)
batches 100, loss 10.443 (re: 202.607, kl: 0.329084)
batches 200, loss 10.5209 (re: 205.879, kl: 0.238876)
batches 300, loss 10.7779 (re: 209.543, kl: 0.316539)
batches 400, loss 10.6331 (re: 209.626, kl: 0.159781)
batches 500, loss 11.4341 (re: 210.152, kl: 0.975217)
batches 600, loss 10.7045 (re: 208.244, kl: 0.307665)
batches 700, loss 10.4165 (re: 205.119, kl: 0.168993)
batches 800, loss 12.04 (re: 211.176, kl: 1.55918)
batches 900, loss 11.572 (re: 203.735, kl: 1.45813)
epoch 12
batches 0, loss 10.8585 (re: 210.087, kl: 0.372839)
batches 100, loss 11.4326 (re: 209.501, kl: 1.00795)
batches 200, loss 10.905 (re: 206.525, kl: 0.609194)
batches 300, loss 10.674 (re: 203.941, kl: 0.502036)
batches 400, loss 10.5201 (re: 203.631, kl: 0.356374)
batches 500, loss 10.5898 (re: 206.2, kl: 0.294546)
batches 600, loss 10.5839 (re: 207.287, kl: 0.231088)
batches 700, loss 10.6667 (re: 209.264, kl: 0.214187)
batches 800, loss 10.5333 (re: 205.153, kl: 0.290213)
batches 900, loss 10.2737 (re: 201.302, kl: 0.219514)
epoch 13
batches 0, loss 10.1441 (re: 200.083, kl: 0.147301)
batches 100, loss 11.5479 (re: 211.203, kl: 1.03974)
batches 200, loss 10.6161 (re: 207.406, kl: 0.258751)
batches 300, loss 10.1027 (re: 199.26, kl: 0.147033)
batches 400, loss 10.4845 (re: 207.175, kl: 0.132337)
batches 500, loss 10.6476 (re: 208.912, kl: 0.212609)
batches 600, loss 10.665 (re: 210.358, kl: 0.154884)
batches 700, loss 10.5403 (re: 206.467, kl: 0.228353)
batches 800, loss 10.4514 (re: 205.743, kl: 0.172883)
batches 900, loss 10.9684 (re: 206.861, kl: 0.658275)
epoch 14
batches 0, loss 10.9527 (re: 209.251, kl: 0.515976)
batches 100, loss 10.4915 (re: 204.833, kl: 0.263022)
batches 200, loss 19.0693 (re: 211.121, kl: 8.96133)
batches 300, loss 11.4803 (re: 199.884, kl: 1.5643)
batches 400, loss 10.4419 (re: 198.011, kl: 0.569847)
batches 500, loss 13.2714 (re: 203.733, kl: 3.24711)
batches 600, loss 10.9605 (re: 207.867, kl: 0.597022)
batches 700, loss 10.5049 (re: 201.916, kl: 0.430629)
batches 800, loss 10.4632 (re: 201.395, kl: 0.414191)
batches 900, loss 10.5823 (re: 204.263, kl: 0.388552)
epoch 15
batches 0, loss 10.7657 (re: 205.898, kl: 0.495546)
batches 100, loss 10.5267 (re: 205.282, kl: 0.276368)
batches 200, loss 10.3858 (re: 202.944, kl: 0.251105)
batches 300, loss 10.8087 (re: 211.01, kl: 0.271783)
batches 400, loss 11.1222 (re: 197.878, kl: 1.29298)
batches 500, loss 11.3748 (re: 213.094, kl: 0.757981)
batches 600, loss 11.0659 (re: 207.804, kl: 0.711302)
batches 700, loss 11.1685 (re: 214.066, kl: 0.489663)
batches 800, loss 10.6046 (re: 202.089, kl: 0.526468)
batches 900, loss 11.0039 (re: 214.628, kl: 0.286881)
epoch 16
batches 0, loss 10.3296 (re: 202.233, kl: 0.229401)
batches 100, loss 10.4125 (re: 204.515, kl: 0.196581)
batches 200, loss 10.5078 (re: 204.052, kl: 0.321254)
batches 300, loss 10.4891 (re: 203.611, kl: 0.324744)
batches 400, loss 10.426 (re: 203.434, kl: 0.267661)
batches 500, loss 10.9917 (re: 215.536, kl: 0.226186)
batches 600, loss 10.7415 (re: 208.746, kl: 0.320237)
batches 700, loss 10.6238 (re: 209.834, kl: 0.139063)
batches 800, loss 10.3651 (re: 204.958, kl: 0.123349)
batches 900, loss 10.3335 (re: 203.172, kl: 0.184065)
epoch 17
batches 0, loss 10.3505 (re: 202.99, kl: 0.211555)
batches 100, loss 10.1803 (re: 201.692, kl: 0.10074)
batches 200, loss 10.6269 (re: 209.285, kl: 0.171178)
batches 300, loss 11.4015 (re: 209.786, kl: 0.960265)
batches 400, loss 12.4162 (re: 211.492, kl: 1.93851)
batches 500, loss 10.4243 (re: 201.128, kl: 0.387262)
batches 600, loss 10.4261 (re: 202.998, kl: 0.290723)
batches 700, loss 11.0565 (re: 211.559, kl: 0.503723)
batches 800, loss 10.887 (re: 213.671, kl: 0.214147)
batches 900, loss 9.90423 (re: 195.101, kl: 0.157017)
epoch 18
batches 0, loss 10.5847 (re: 209.182, kl: 0.132162)
batches 100, loss 22.4455 (re: 203.054, kl: 12.9398)
batches 200, loss 11.3203 (re: 205.887, kl: 1.07996)
batches 300, loss 12.1512 (re: 210.619, kl: 1.70555)
batches 400, loss 11.262 (re: 204.874, kl: 1.07186)
batches 500, loss 10.6899 (re: 200.047, kl: 0.723736)
batches 600, loss 11.013 (re: 209.896, kl: 0.545432)
batches 700, loss 10.8079 (re: 205.691, kl: 0.550935)
batches 800, loss 10.6689 (re: 205.183, kl: 0.4313)
batches 900, loss 10.9315 (re: 211.856, kl: 0.356585)
epoch 19
batches 0, loss 10.9545 (re: 212.168, kl: 0.36429)
batches 100, loss 10.7775 (re: 208.989, kl: 0.345273)
batches 200, loss 10.639 (re: 207.629, kl: 0.271087)
batches 300, loss 10.4716 (re: 205.605, kl: 0.201427)
batches 400, loss 10.2565 (re: 201.128, kl: 0.210631)
batches 500, loss 10.3587 (re: 203.386, kl: 0.199329)
batches 600, loss 10.6763 (re: 208.264, kl: 0.2769)
batches 700, loss 10.5889 (re: 208.16, kl: 0.190468)
batches 800, loss 10.4641 (re: 206.193, kl: 0.162623)
batches 900, loss 10.3618 (re: 204.906, kl: 0.122622)

Process finished with exit code 0

