/home/snowkylin/anaconda2/envs/tensorflow/bin/python /home/snowkylin/桌面/rnn_vae/train.py
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GeForce GTX 1060 6GB
major: 6 minor: 1 memoryClockRate (GHz) 1.8095
pciBusID 0000:01:00.0
Total memory: 5.96GiB
Free memory: 5.54GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)
epoch 0
batches 0, loss 156.323 (re: 592.762, kl: 47.2128)
batches 100, loss 40.4986 (re: 202.247, kl: 0.0614886)
batches 200, loss 40.7777 (re: 203.804, kl: 0.0210506)
batches 300, loss 41.6299 (re: 208.102, kl: 0.0119366)
batches 400, loss 41.024 (re: 205.088, kl: 0.00813099)
batches 500, loss 42.5329 (re: 212.639, kl: 0.00626635)
batches 600, loss 39.6915 (re: 198.439, kl: 0.00444204)
batches 700, loss 41.1335 (re: 205.653, kl: 0.00356354)
batches 800, loss 40.8339 (re: 204.157, kl: 0.00303051)
batches 900, loss 40.9966 (re: 204.973, kl: 0.00250895)
epoch 1
batches 0, loss 41.211 (re: 206.047, kl: 0.00210928)
batches 100, loss 40.8556 (re: 204.271, kl: 0.00178732)
batches 200, loss 40.7605 (re: 203.796, kl: 0.00155565)
batches 300, loss 39.864 (re: 199.314, kl: 0.0013583)
batches 400, loss 41.5023 (re: 207.506, kl: 0.00138739)
batches 500, loss 40.9786 (re: 204.888, kl: 0.00117371)
batches 600, loss 41.1954 (re: 205.972, kl: 0.00112871)
batches 700, loss 41.9302 (re: 209.646, kl: 0.00110857)
batches 800, loss 43.3806 (re: 216.898, kl: 0.00120209)
batches 900, loss 41.5917 (re: 207.954, kl: 0.00111349)
epoch 2
batches 0, loss 43.3738 (re: 216.865, kl: 0.00106609)
batches 100, loss 41.8251 (re: 209.12, kl: 0.00128695)
batches 200, loss 42.1961 (re: 210.976, kl: 0.00115208)
batches 300, loss 42.1186 (re: 210.589, kl: 0.000929737)
batches 400, loss 40.9623 (re: 204.807, kl: 0.00103065)
batches 500, loss 40.8109 (re: 204.051, kl: 0.000939522)
batches 600, loss 40.4812 (re: 202.402, kl: 0.00106119)
batches 700, loss 42.5405 (re: 212.697, kl: 0.00125969)
batches 800, loss 42.3436 (re: 211.714, kl: 0.00111115)
batches 900, loss 41.3182 (re: 206.587, kl: 0.00115324)
epoch 3
batches 0, loss 42.5629 (re: 212.808, kl: 0.00166248)
batches 100, loss 41.2614 (re: 206.299, kl: 0.00202225)
batches 200, loss 39.9083 (re: 199.535, kl: 0.00161308)
batches 300, loss 41.8625 (re: 209.298, kl: 0.00358915)
batches 400, loss 41.4188 (re: 207.072, kl: 0.00567448)
batches 500, loss 39.0196 (re: 195.024, kl: 0.0185749)
batches 600, loss 41.4167 (re: 206.173, kl: 0.227681)
batches 700, loss 38.9664 (re: 193.416, kl: 0.353906)
batches 800, loss 36.9638 (re: 183.1, kl: 0.429638)
batches 900, loss 35.149 (re: 174.654, kl: 0.272786)
epoch 4
batches 0, loss 32.8089 (re: 162.948, kl: 0.274093)
batches 100, loss 32.4066 (re: 160.477, kl: 0.389016)
batches 200, loss 32.3566 (re: 160.215, kl: 0.392122)
batches 300, loss 31.3875 (re: 155.133, kl: 0.451076)
batches 400, loss 30.5964 (re: 151.226, kl: 0.439041)
batches 500, loss 30.1679 (re: 148.929, kl: 0.477755)
batches 600, loss 29.0477 (re: 143.543, kl: 0.423943)
batches 700, loss 28.1018 (re: 138.559, kl: 0.487605)
batches 800, loss 25.3949 (re: 125.226, kl: 0.436989)
batches 900, loss 26.0166 (re: 128.491, kl: 0.397962)
epoch 5
batches 0, loss 25.9909 (re: 128.415, kl: 0.384983)
batches 100, loss 25.9497 (re: 128.246, kl: 0.375613)
batches 200, loss 25.5965 (re: 126.573, kl: 0.35225)
batches 300, loss 26.3931 (re: 130.51, kl: 0.363756)
batches 400, loss 23.6185 (re: 116.701, kl: 0.347962)
batches 500, loss 24.3223 (re: 119.917, kl: 0.423598)
batches 600, loss 24.9493 (re: 123.366, kl: 0.344989)
batches 700, loss 23.6705 (re: 116.987, kl: 0.341317)
batches 800, loss 24.7802 (re: 122.663, kl: 0.309454)
batches 900, loss 22.6729 (re: 112.026, kl: 0.334716)
epoch 6
batches 0, loss 23.0389 (re: 113.666, kl: 0.382033)
batches 100, loss 22.1575 (re: 109.526, kl: 0.315247)
batches 200, loss 24.528 (re: 121.34, kl: 0.32491)
batches 300, loss 23.2484 (re: 114.915, kl: 0.331756)
batches 400, loss 22.6091 (re: 111.738, kl: 0.326937)
batches 500, loss 21.8232 (re: 107.913, kl: 0.30079)
batches 600, loss 21.97 (re: 108.711, kl: 0.284713)
batches 700, loss 21.7213 (re: 107.513, kl: 0.273415)
batches 800, loss 22.8347 (re: 112.874, kl: 0.32502)
batches 900, loss 22.1049 (re: 109.368, kl: 0.289188)
epoch 7
batches 0, loss 20.5185 (re: 101.425, kl: 0.291841)
batches 100, loss 20.9245 (re: 103.57, kl: 0.263139)
batches 200, loss 21.8212 (re: 107.931, kl: 0.293712)
batches 300, loss 21.6203 (re: 107.1, kl: 0.250355)
batches 400, loss 20.1424 (re: 99.7294, kl: 0.245711)
batches 500, loss 22.5159 (re: 111.365, kl: 0.303594)
batches 600, loss 20.5869 (re: 101.792, kl: 0.285636)
batches 700, loss 21.0495 (re: 104.066, kl: 0.295534)
batches 800, loss 19.2542 (re: 95.0841, kl: 0.296749)
batches 900, loss 20.9431 (re: 103.541, kl: 0.293584)
epoch 8
batches 0, loss 20.1926 (re: 99.7612, kl: 0.300512)
batches 100, loss 20.395 (re: 101.029, kl: 0.236557)
batches 200, loss 19.9766 (re: 98.7485, kl: 0.283644)
batches 300, loss 20.7138 (re: 102.344, kl: 0.306229)
batches 400, loss 21.2653 (re: 105.314, kl: 0.253067)
batches 500, loss 20.4481 (re: 101.208, kl: 0.2581)
batches 600, loss 19.0093 (re: 94.1958, kl: 0.212713)
batches 700, loss 20.3252 (re: 100.583, kl: 0.260856)
batches 800, loss 19.3817 (re: 96.0631, kl: 0.211406)
batches 900, loss 19.7584 (re: 97.9165, kl: 0.218847)
epoch 9
batches 0, loss 19.3592 (re: 95.9096, kl: 0.221661)
batches 100, loss 19.2229 (re: 95.1683, kl: 0.236607)
batches 200, loss 19.1277 (re: 94.651, kl: 0.246823)
batches 300, loss 19.6125 (re: 97.0771, kl: 0.246375)
batches 400, loss 19.2935 (re: 95.4823, kl: 0.24628)
batches 500, loss 19.271 (re: 95.4109, kl: 0.236045)
batches 600, loss 19.6902 (re: 97.6487, kl: 0.200541)
batches 700, loss 19.5185 (re: 96.8218, kl: 0.192701)
batches 800, loss 17.7953 (re: 88.1562, kl: 0.205069)
batches 900, loss 20.0413 (re: 99.3749, kl: 0.207842)
epoch 10
batches 0, loss 18.9837 (re: 93.9801, kl: 0.234669)
batches 100, loss 18.3182 (re: 90.7351, kl: 0.213925)
batches 200, loss 19.0198 (re: 94.2146, kl: 0.221078)
batches 300, loss 18.5399 (re: 91.8871, kl: 0.203122)
batches 400, loss 19.1631 (re: 94.8516, kl: 0.241018)
batches 500, loss 20.1705 (re: 99.9928, kl: 0.214977)
batches 600, loss 17.9935 (re: 89.0775, kl: 0.222545)
batches 700, loss 17.068 (re: 84.5054, kl: 0.208699)
batches 800, loss 19.2882 (re: 95.6303, kl: 0.202719)
batches 900, loss 18.6425 (re: 92.3979, kl: 0.203702)
epoch 11
batches 0, loss 18.6227 (re: 92.3206, kl: 0.198171)
batches 100, loss 18.647 (re: 92.3478, kl: 0.221757)
batches 200, loss 18.5754 (re: 92.0651, kl: 0.202943)
batches 300, loss 18.9262 (re: 93.9214, kl: 0.177439)
batches 400, loss 17.8031 (re: 88.3402, kl: 0.168824)
batches 500, loss 18.4971 (re: 91.6419, kl: 0.210916)
batches 600, loss 17.5715 (re: 86.9625, kl: 0.223773)
batches 700, loss 18.9357 (re: 93.9026, kl: 0.194025)
batches 800, loss 18.9739 (re: 94.1625, kl: 0.176795)
batches 900, loss 18.3406 (re: 90.9139, kl: 0.197294)
epoch 12
batches 0, loss 18.3435 (re: 90.9645, kl: 0.188254)
batches 100, loss 18.5157 (re: 91.7699, kl: 0.202137)
batches 200, loss 17.5965 (re: 87.328, kl: 0.163603)
batches 300, loss 17.7649 (re: 88.085, kl: 0.184847)
batches 400, loss 18.6394 (re: 92.4175, kl: 0.194914)
batches 500, loss 17.7883 (re: 88.176, kl: 0.191405)
batches 600, loss 18.0543 (re: 89.5036, kl: 0.19199)
batches 700, loss 18.6204 (re: 92.3789, kl: 0.180806)
batches 800, loss 18.3497 (re: 91.0173, kl: 0.182838)
batches 900, loss 18.3233 (re: 90.8733, kl: 0.185828)
epoch 13
batches 0, loss 17.1152 (re: 84.7122, kl: 0.216012)
batches 100, loss 18.5654 (re: 92.0339, kl: 0.198253)
batches 200, loss 18.7662 (re: 93.1719, kl: 0.164826)
batches 300, loss 17.202 (re: 85.2549, kl: 0.188796)
batches 400, loss 17.5482 (re: 86.9395, kl: 0.200364)
batches 500, loss 17.8235 (re: 88.4443, kl: 0.16829)
batches 600, loss 17.2284 (re: 85.3766, kl: 0.191385)
batches 700, loss 18.7838 (re: 93.1141, kl: 0.201174)
batches 800, loss 18.1287 (re: 89.9226, kl: 0.180195)
batches 900, loss 17.2904 (re: 85.7471, kl: 0.176199)
epoch 14
batches 0, loss 17.7935 (re: 88.1179, kl: 0.212375)
batches 100, loss 19.1184 (re: 94.7675, kl: 0.206103)
batches 200, loss 18.728 (re: 92.8869, kl: 0.188282)
batches 300, loss 18.4724 (re: 91.6752, kl: 0.171745)
batches 400, loss 17.2956 (re: 85.7075, kl: 0.192645)
batches 500, loss 18.8854 (re: 93.5981, kl: 0.207273)
batches 600, loss 17.45 (re: 86.4379, kl: 0.203029)
batches 700, loss 17.3575 (re: 86.1857, kl: 0.150507)
batches 800, loss 17.5859 (re: 87.2289, kl: 0.175173)
batches 900, loss 18.0245 (re: 89.3578, kl: 0.19118)
epoch 15
batches 0, loss 17.4261 (re: 86.3873, kl: 0.185743)
batches 100, loss 18.2093 (re: 90.2682, kl: 0.194594)
batches 200, loss 16.424 (re: 81.4578, kl: 0.165593)
batches 300, loss 16.8198 (re: 83.459, kl: 0.160033)
batches 400, loss 17.5775 (re: 87.1311, kl: 0.189135)
batches 500, loss 16.9759 (re: 84.1594, kl: 0.180081)
batches 600, loss 17.7617 (re: 88.1095, kl: 0.174753)
batches 700, loss 16.8645 (re: 83.6284, kl: 0.173477)
batches 800, loss 17.272 (re: 85.6679, kl: 0.173071)
batches 900, loss 18.1523 (re: 90.0362, kl: 0.181291)
epoch 16
batches 0, loss 16.9581 (re: 84.0826, kl: 0.176939)
batches 100, loss 16.5212 (re: 81.9359, kl: 0.167497)
batches 200, loss 17.6937 (re: 87.8425, kl: 0.156468)
batches 300, loss 17.9501 (re: 89.093, kl: 0.164328)
batches 400, loss 16.8952 (re: 83.8622, kl: 0.153493)
batches 500, loss 17.3506 (re: 86.0069, kl: 0.186485)
batches 600, loss 17.0462 (re: 84.5473, kl: 0.170971)
batches 700, loss 17.5927 (re: 87.2273, kl: 0.184099)
batches 800, loss 17.396 (re: 86.3037, kl: 0.169042)
batches 900, loss 17.216 (re: 85.3628, kl: 0.179282)
epoch 17
batches 0, loss 15.6089 (re: 77.3907, kl: 0.163388)
batches 100, loss 17.3407 (re: 86.0484, kl: 0.163815)
batches 200, loss 16.9584 (re: 84.1302, kl: 0.165425)
batches 300, loss 16.6544 (re: 82.5769, kl: 0.173724)
batches 400, loss 15.9403 (re: 79.0568, kl: 0.16117)
batches 500, loss 17.2222 (re: 85.3967, kl: 0.17852)
batches 600, loss 17.4837 (re: 86.7169, kl: 0.175392)
batches 700, loss 17.8344 (re: 88.5412, kl: 0.157746)
batches 800, loss 17.8018 (re: 88.3598, kl: 0.162322)
batches 900, loss 18.4074 (re: 91.2937, kl: 0.185792)
epoch 18
batches 0, loss 17.2512 (re: 85.6196, kl: 0.159125)
batches 100, loss 17.1571 (re: 85.088, kl: 0.174417)
batches 200, loss 16.22 (re: 80.4214, kl: 0.169591)
batches 300, loss 16.2082 (re: 80.35, kl: 0.172745)
batches 400, loss 16.5531 (re: 82.1105, kl: 0.163765)
batches 500, loss 16.6054 (re: 82.4247, kl: 0.150582)
batches 600, loss 16.9878 (re: 84.2743, kl: 0.166209)
batches 700, loss 17.5361 (re: 86.9554, kl: 0.181221)
batches 800, loss 16.411 (re: 81.424, kl: 0.157706)
batches 900, loss 16.0015 (re: 79.3303, kl: 0.169307)
epoch 19
batches 0, loss 16.5946 (re: 82.2766, kl: 0.174136)
batches 100, loss 16.9542 (re: 84.1197, kl: 0.162877)
batches 200, loss 16.8129 (re: 83.3155, kl: 0.187217)
batches 300, loss 17.856 (re: 88.6041, kl: 0.168949)
batches 400, loss 17.5299 (re: 86.9255, kl: 0.181007)
batches 500, loss 17.3054 (re: 85.8612, kl: 0.166403)
batches 600, loss 17.0785 (re: 84.835, kl: 0.139345)
batches 700, loss 16.458 (re: 81.7068, kl: 0.145774)
batches 800, loss 17.879 (re: 88.7433, kl: 0.16287)
batches 900, loss 16.5268 (re: 81.8981, kl: 0.183955)

Process finished with exit code 0

